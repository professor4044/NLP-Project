{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/professor4044/NLP-Project/blob/main/NLPProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK : 1 : Load Python Libraries"
      ],
      "metadata": {
        "id": "jpaiPivXqMgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, time, random, string, urllib.request\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet as wn\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support,\n",
        "    classification_report, confusion_matrix,\n",
        "    roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "import sklearn, matplotlib\n"
      ],
      "metadata": {
        "id": "O2cLoG-nqZjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK : 2 : Load NLTK package"
      ],
      "metadata": {
        "id": "27BPNXHfqpkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "packages = [\n",
        "    \"punkt\", \"stopwords\", \"wordnet\", \"omw-1.4\",\n",
        "    \"averaged_perceptron_tagger\", \"averaged_perceptron_tagger_eng\",\n",
        "    \"punkt_tab\"\n",
        "]\n",
        "\n",
        "for p in packages:\n",
        "    try:\n",
        "        nltk.download(p, quiet=True)\n",
        "    except Exception as e:\n",
        "        print(f\"NLTK download failed for {p}: {e}\")\n",
        "\n",
        "STOP_WORDS = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "print(\"NLTK done\")\n"
      ],
      "metadata": {
        "id": "89wVAWM-q20g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK : 3 :Load dataset"
      ],
      "metadata": {
        "id": "YIX3fDpUK74c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "dataset = load_dataset(\"csv\", data_files=\"fake.csv\", split=\"train[:10000]\")\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "print(df.head(3))\n",
        "\n",
        "print(\"Label distribution:\", Counter(df[\"type\"].tolist()))\n",
        "\n",
        "print(\"Total rows:\", len(df))\n"
      ],
      "metadata": {
        "id": "nDq74UWXLG60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:4:Tokenization"
      ],
      "metadata": {
        "id": "a_yark5bPjmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# ১. ফাংশন ডিফাইন করা (ছবির স্টাইলে)\n",
        "def tokenize(column_data):\n",
        "    return [word_tokenize(str(t)) for t in column_data]\n",
        "\n",
        "# ২. শুধুমাত্র 'text' কলাম টোকেনাইজ করা\n",
        "tokenized_text = tokenize(df['text'])\n",
        "\n",
        "# ৩. আউটপুট দেখা (প্রথম নিউজের প্রথম ৩০টি টোকেন)\n",
        "print(tokenized_text[0][:50])"
      ],
      "metadata": {
        "id": "BykpCWlLPwj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:5:Case folding"
      ],
      "metadata": {
        "id": "NyXL22AWfvfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# ১. শুধু 'text' কলামটি টোকেনাইজ করে একটি লিস্টে রাখা\n",
        "tokenized_text = [word_tokenize(str(news)) for news in df['text']]\n",
        "\n",
        "# ২. আউটপুট দেখা (ছবির স্টাইলে - প্রথম খবরের প্রথম ৩০টি শব্দ)\n",
        "print(tokenized_text[0][:30])"
      ],
      "metadata": {
        "id": "QmZpRAQCf3Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:6:Punctuation Removal"
      ],
      "metadata": {
        "id": "vbg9VZlTnA6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# ১. ফাংশন যা একবারে Punctuation এবং Special Character রিমুভ করবে\n",
        "def clean_tokens(doc_tokens):\n",
        "    clean_doc = []\n",
        "    for token in doc_tokens:\n",
        "        # শুধু a-z, A-Z এবং 0-9 রাখা হবে।\n",
        "        # এর মানে Punctuation এবং Special Char সব অটোমেটিক মুছে যাবে।\n",
        "        new_token = re.sub(r'[^a-zA-Z0-9]', '', token)\n",
        "\n",
        "        # মুছতে মুছতে যদি শব্দটি ফাঁকা না হয়ে যায়, তবেই লিস্টে রাখা হবে\n",
        "        if new_token != '':\n",
        "            clean_doc.append(new_token)\n",
        "    return clean_doc\n",
        "\n",
        "# ২. আপনার টোকেনাইজড টেক্সটের ওপর এই ফাংশনটি চালানো\n",
        "# (মনে রাখবেন: 'tokenized_text' হলো আপনার আগের ধাপে পাওয়া টোকেন লিস্ট)\n",
        "final_cleaned_text = [clean_tokens(tokens) for tokens in tokenized_text]\n",
        "\n",
        "# ৩. আউটপুট দেখা (প্রথম খবরের প্রথম ৩০টি ক্লিন টোকেন)\n",
        "print(final_cleaned_text[0][:30])"
      ],
      "metadata": {
        "id": "6BgLPXYknJ-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:7:Stop word removal"
      ],
      "metadata": {
        "id": "KkfAdHSFoRrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# ১. স্টপওয়ার্ডস লোড করা\n",
        "#nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# ২. ফাংশন তৈরি\n",
        "def remove_stopwords(tokens_list):\n",
        "    # স্টপ ওয়ার্ড লিস্টে নেই এমন শব্দগুলোই শুধু রাখা হবে\n",
        "    return [word for word in tokens_list if word not in stop_words]\n",
        "\n",
        "# ৩. আপনার ক্লিন করা টেক্সটের ওপর এটি অ্যাপ্লাই করা\n",
        "# (মনে রাখবেন: 'final_cleaned_text' হলো আপনার আগের ধাপের স্পেশাল ক্যারেক্টার রিমুভ করা লিস্ট)\n",
        "text_without_stopwords = [remove_stopwords(tokens) for tokens in final_cleaned_text]\n",
        "\n",
        "# ৪. আউটপুট দেখা (সরাসরি লিস্ট)\n",
        "print(text_without_stopwords[0][:30])"
      ],
      "metadata": {
        "id": "3DxGeZ_BqGEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:8:Lemmatization"
      ],
      "metadata": {
        "id": "KN5sYXoDPVBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# এই লাইনের শুরুতে # দিয়ে কমেন্ট করে দিন, কারণ এটি অলরেডি নামানো আছে\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(token_list):\n",
        "    return [lemmatizer.lemmatize(token) for token in token_list]\n",
        "\n",
        "lemmatized_output = [lemmatize_text(tokens) for tokens in text_without_stopwords]\n",
        "\n",
        "print(lemmatized_output[0][:30])"
      ],
      "metadata": {
        "id": "wkd5561kPhTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:9:Synonym Substitution"
      ],
      "metadata": {
        "id": "6_al5LqVSirP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# ১. ছবির মেথড অনুযায়ী ফাংশন তৈরি\n",
        "def synonym_substitution(tokens_list):\n",
        "    new_sentence = []\n",
        "\n",
        "    for word in tokens_list:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "\n",
        "        # যদি শব্দটির কোনো সিনোনিম বা সমার্থক শব্দ ডিকশনারিতে পাওয়া যায়\n",
        "        if synonyms:\n",
        "            # প্রথম সিনোনিমটি খুঁজে বের করা (এটিই হবে আমাদের replacement_word)\n",
        "            new_word = synonyms[0].lemmas()[0].name()\n",
        "\n",
        "            # যদি সিনোনিমটি মূল শব্দের চেয়ে আলাদা হয় এবং এতে কোনো আন্ডারস্কোর (_) না থাকে\n",
        "            if new_word != word and \"_\" not in new_word:\n",
        "                new_sentence.append(new_word) # রিপ্লেস করা হলো\n",
        "            else:\n",
        "                new_sentence.append(word)     # আগেরটাই রাখা হলো\n",
        "        else:\n",
        "            new_sentence.append(word)         # সিনোনিম না পেলে যা ছিল তাই থাকবে\n",
        "\n",
        "    return new_sentence\n",
        "\n",
        "# ২. আপনার লেমাটাইজড ডাটার ওপর এটি চালানো\n",
        "# 'lemmatized_output' হলো আপনার আগের ধাপের ডাটা\n",
        "synonym_replaced_data = [synonym_substitution(tokens) for tokens in lemmatized_output]\n",
        "\n",
        "# ৩. আউটপুট দেখা\n",
        "print(synonym_replaced_data[0][:30])"
      ],
      "metadata": {
        "id": "pgb3wEe4SnQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:10:Split (Train , Test/Validation)"
      ],
      "metadata": {
        "id": "-oveGPczViMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ১. টোকেনগুলোকে জোড়া লাগিয়ে আবার বাক্যে রূপান্তর করা\n",
        "# 'synonym_replaced_data' হলো আপনার সর্বশেষ প্রসেস করা ডেটা\n",
        "X_text = [\" \".join(tokens) for tokens in synonym_replaced_data]\n",
        "\n",
        "# ২. ফিচার এবং লেবেল ঠিক করা\n",
        "X = X_text\n",
        "y = df['type']\n",
        "\n",
        "# ৩. Train-Test Split (৮০% ট্রেনিং, ২০% টেস্টিং)\n",
        "# test_size=0.2 মানে ২০% ডেটা পরীক্ষার জন্য\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ৪. রেজাল্ট দেখা\n",
        "print(\"Total Data:\", len(X))\n",
        "print(\"Training Data Size (80%):\", len(X_train))\n",
        "print(\"Testing Data Size (20%):\", len(X_test))"
      ],
      "metadata": {
        "id": "iYCpzABpybRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:11:Using TF-IDF"
      ],
      "metadata": {
        "id": "PYWynl7u5Ifj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ১. min_df=5 : যে শব্দটি অন্তত ৫টি আলাদা নিউজে আসেনি, সেটি বাদ যাবে (Typo বা ফালতু শব্দ কমে যাবে)\n",
        "# ২. ngram_range=(1, 3) : এখনো ১-৩ শব্দের জোড়া রাখা হলো যাতে কনটেক্সট থাকে\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=5)\n",
        "\n",
        "# ৩. Training Data-র ওপর fit এবং transform করা\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# ৪. Testing Data-র ওপর transform করা\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# ৫. আউটপুট চেক করা\n",
        "print(\"Training Data Shape:\", X_train_tfidf.shape)\n",
        "print(\"Testing Data Shape:\", X_test_tfidf.shape)"
      ],
      "metadata": {
        "id": "0nlOFREU5NLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:12:Applying Naïve Bayes Algorithm"
      ],
      "metadata": {
        "id": "99oxZGfYWn5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# ১. মডেল তৈরি করা\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# ২. ট্রেনিং ডেটা দিয়ে মডেল ট্রেইন করা\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# ৩. টেস্ট ডেটা দিয়ে প্রেডিকশন করা\n",
        "y_pred = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "# ৪. আউটপুট (আপনার চাওয়া অনুযায়ী শুধু মেসেজ)\n",
        "print(\"Naive Bayes algorithm has been applied successfully.\")"
      ],
      "metadata": {
        "id": "yGS2FjmxWyjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:13:Confusion matrix & Curve (ROC, AUC)"
      ],
      "metadata": {
        "id": "lXWSadl2Yw61"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EYdxfVDQY4ir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}