{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6BPIUQ0qzcmHK3Ebjxwlt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/professor4044/NLP-Project/blob/main/NLPProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK : 1 : Load Python Libraries"
      ],
      "metadata": {
        "id": "jpaiPivXqMgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, time, random, string, urllib.request\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet as wn\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support,\n",
        "    classification_report, confusion_matrix,\n",
        "    roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "import sklearn, matplotlib\n"
      ],
      "metadata": {
        "id": "O2cLoG-nqZjX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK : 2 : Load NLTK package"
      ],
      "metadata": {
        "id": "27BPNXHfqpkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "packages = [\n",
        "    \"punkt\", \"stopwords\", \"wordnet\", \"omw-1.4\",\n",
        "    \"averaged_perceptron_tagger\", \"averaged_perceptron_tagger_eng\",\n",
        "    \"punkt_tab\"\n",
        "]\n",
        "\n",
        "for p in packages:\n",
        "    try:\n",
        "        nltk.download(p, quiet=True)\n",
        "    except Exception as e:\n",
        "        print(f\"NLTK download failed for {p}: {e}\")\n",
        "\n",
        "STOP_WORDS = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "print(\"NLTK done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89wVAWM-q20g",
        "outputId": "48b332ce-b245-4d62-9cc3-9ce0ff2efc07"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK done\n"
          ]
        }
      ]
    }
  ]
}