{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+y31nk0QAd5raU10MVr4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/professor4044/NLP-Project/blob/main/NLPProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK : 1 : Load Python Libraries"
      ],
      "metadata": {
        "id": "jpaiPivXqMgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, time, random, string, urllib.request\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet as wn\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support,\n",
        "    classification_report, confusion_matrix,\n",
        "    roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "import sklearn, matplotlib\n"
      ],
      "metadata": {
        "id": "O2cLoG-nqZjX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK : 2 : Load NLTK package"
      ],
      "metadata": {
        "id": "27BPNXHfqpkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "packages = [\n",
        "    \"punkt\", \"stopwords\", \"wordnet\", \"omw-1.4\",\n",
        "    \"averaged_perceptron_tagger\", \"averaged_perceptron_tagger_eng\",\n",
        "    \"punkt_tab\"\n",
        "]\n",
        "\n",
        "for p in packages:\n",
        "    try:\n",
        "        nltk.download(p, quiet=True)\n",
        "    except Exception as e:\n",
        "        print(f\"NLTK download failed for {p}: {e}\")\n",
        "\n",
        "STOP_WORDS = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "print(\"NLTK done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89wVAWM-q20g",
        "outputId": "8889d0d1-7177-4a86-d32a-8336cfcff141"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK : 3 :Load dataset"
      ],
      "metadata": {
        "id": "YIX3fDpUK74c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "dataset = load_dataset(\"csv\", data_files=\"fake.csv\", split=\"train[:10000]\")\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "print(df.head(3))\n",
        "\n",
        "print(\"Label distribution:\", Counter(df[\"type\"].tolist()))\n",
        "\n",
        "print(\"Total rows:\", len(df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDq74UWXLG60",
        "outputId": "2c0c4096-8349-4e64-8877-8d28c7b982f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       uuid  ord_in_thread  \\\n",
            "0  6a175f46bcd24d39b3e962ad0f29936721db70db              0   \n",
            "1  2bdc29d12605ef9cf3f09f9875040a7113be5d5b              0   \n",
            "2  c70e149fdd53de5e61c29281100b9de0ed268bc3              0   \n",
            "\n",
            "                 author                      published  \\\n",
            "0     Barracuda Brigade  2016-10-26T21:41:00.000+03:00   \n",
            "1  reasoning with facts  2016-10-29T08:47:11.259+03:00   \n",
            "2     Barracuda Brigade  2016-10-31T01:41:49.479+02:00   \n",
            "\n",
            "                                               title  \\\n",
            "0  Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
            "1  Re: Why Did Attorney General Loretta Lynch Ple...   \n",
            "2  BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
            "\n",
            "                                                text language  \\\n",
            "0  Print They should pay all the back all the mon...  english   \n",
            "1  Why Did Attorney General Loretta Lynch Plead T...  english   \n",
            "2  Red State : \\nFox News Sunday reported this mo...  english   \n",
            "\n",
            "                         crawled             site_url country  domain_rank  \\\n",
            "0  2016-10-27T01:49:27.168+03:00  100percentfedup.com      US      25689.0   \n",
            "1  2016-10-29T08:47:11.259+03:00  100percentfedup.com      US      25689.0   \n",
            "2  2016-10-31T01:41:49.479+02:00  100percentfedup.com      US      25689.0   \n",
            "\n",
            "                                        thread_title  spam_score  \\\n",
            "0  Muslims BUSTED: They Stole Millions In Gov’t B...         0.0   \n",
            "1  Re: Why Did Attorney General Loretta Lynch Ple...         0.0   \n",
            "2  BREAKING: Weiner Cooperating With FBI On Hilla...         0.0   \n",
            "\n",
            "                                        main_img_url  replies_count  \\\n",
            "0  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
            "1  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
            "2  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
            "\n",
            "   participants_count  likes  comments  shares  type  \n",
            "0                   1      0         0       0  bias  \n",
            "1                   1      0         0       0  bias  \n",
            "2                   1      0         0       0  bias  \n",
            "Label distribution: Counter({'bs': 8693, 'conspiracy': 430, 'hate': 246, 'bias': 243, 'satire': 146, 'state': 121, 'junksci': 102, 'fake': 19})\n",
            "Total rows: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:4:Tokenization"
      ],
      "metadata": {
        "id": "a_yark5bPjmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# ১. ফাংশন ডিফাইন করা (ছবির স্টাইলে)\n",
        "def tokenize(column_data):\n",
        "    return [word_tokenize(str(t)) for t in column_data]\n",
        "\n",
        "# ২. শুধুমাত্র 'text' কলাম টোকেনাইজ করা\n",
        "tokenized_text = tokenize(df['text'])\n",
        "\n",
        "# ৩. আউটপুট দেখা (প্রথম নিউজের প্রথম ৩০টি টোকেন)\n",
        "print(tokenized_text[0][:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BykpCWlLPwj-",
        "outputId": "6f134793-684a-47e7-93a0-f3cc57049120"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Print', 'They', 'should', 'pay', 'all', 'the', 'back', 'all', 'the', 'money', 'plus', 'interest', '.', 'The', 'entire', 'family', 'and', 'everyone', 'who', 'came', 'in', 'with', 'them', 'need', 'to', 'be', 'deported', 'asap', '.', 'Why', 'did', 'it', 'take', 'two', 'years', 'to', 'bust', 'them', '?', 'Here', 'we', 'go', 'again', '…another', 'group', 'stealing', 'from', 'the', 'government', 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:5:Case folding"
      ],
      "metadata": {
        "id": "NyXL22AWfvfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# ১. শুধু 'text' কলামটি টোকেনাইজ করে একটি লিস্টে রাখা\n",
        "tokenized_text = [word_tokenize(str(news)) for news in df['text']]\n",
        "\n",
        "# ২. আউটপুট দেখা (ছবির স্টাইলে - প্রথম খবরের প্রথম ৩০টি শব্দ)\n",
        "print(tokenized_text[0][:30])"
      ],
      "metadata": {
        "id": "QmZpRAQCf3Ts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce8a15c6-fefe-4de1-b029-15ddcdd1df90"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['print', 'they', 'should', 'pay', 'all', 'the', 'back', 'all', 'the', 'money', 'plus', 'interest', '.', 'the', 'entire', 'family', 'and', 'everyone', 'who', 'came', 'in', 'with', 'them', 'need', 'to', 'be', 'deported', 'asap', '.', 'why']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:6:Punctuation Removal"
      ],
      "metadata": {
        "id": "vbg9VZlTnA6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# ১. ফাংশন যা একবারে Punctuation এবং Special Character রিমুভ করবে\n",
        "def clean_tokens(doc_tokens):\n",
        "    clean_doc = []\n",
        "    for token in doc_tokens:\n",
        "        # শুধু a-z, A-Z এবং 0-9 রাখা হবে।\n",
        "        # এর মানে Punctuation এবং Special Char সব অটোমেটিক মুছে যাবে।\n",
        "        new_token = re.sub(r'[^a-zA-Z0-9]', '', token)\n",
        "\n",
        "        # মুছতে মুছতে যদি শব্দটি ফাঁকা না হয়ে যায়, তবেই লিস্টে রাখা হবে\n",
        "        if new_token != '':\n",
        "            clean_doc.append(new_token)\n",
        "    return clean_doc\n",
        "\n",
        "# ২. আপনার টোকেনাইজড টেক্সটের ওপর এই ফাংশনটি চালানো\n",
        "# (মনে রাখবেন: 'tokenized_text' হলো আপনার আগের ধাপে পাওয়া টোকেন লিস্ট)\n",
        "final_cleaned_text = [clean_tokens(tokens) for tokens in tokenized_text]\n",
        "\n",
        "# ৩. আউটপুট দেখা (প্রথম খবরের প্রথম ৩০টি ক্লিন টোকেন)\n",
        "print(final_cleaned_text[0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BgLPXYknJ-w",
        "outputId": "966ef99a-7924-4f06-8427-8ee83934764a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['print', 'they', 'should', 'pay', 'all', 'the', 'back', 'all', 'the', 'money', 'plus', 'interest', 'the', 'entire', 'family', 'and', 'everyone', 'who', 'came', 'in', 'with', 'them', 'need', 'to', 'be', 'deported', 'asap', 'why', 'did', 'it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK:7:Stop word removal"
      ],
      "metadata": {
        "id": "KkfAdHSFoRrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# ১. স্টপওয়ার্ডস লোড করা\n",
        "#nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# ২. ফাংশন তৈরি\n",
        "def remove_stopwords(tokens_list):\n",
        "    # স্টপ ওয়ার্ড লিস্টে নেই এমন শব্দগুলোই শুধু রাখা হবে\n",
        "    return [word for word in tokens_list if word not in stop_words]\n",
        "\n",
        "# ৩. আপনার ক্লিন করা টেক্সটের ওপর এটি অ্যাপ্লাই করা\n",
        "# (মনে রাখবেন: 'final_cleaned_text' হলো আপনার আগের ধাপের স্পেশাল ক্যারেক্টার রিমুভ করা লিস্ট)\n",
        "text_without_stopwords = [remove_stopwords(tokens) for tokens in final_cleaned_text]\n",
        "\n",
        "# ৪. আউটপুট দেখা (সরাসরি লিস্ট)\n",
        "print(text_without_stopwords[0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DxGeZ_BqGEX",
        "outputId": "ea0dcb48-274b-46a5-f145-95f05ea8e14b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['print', 'pay', 'back', 'money', 'plus', 'interest', 'entire', 'family', 'everyone', 'came', 'need', 'deported', 'asap', 'take', 'two', 'years', 'bust', 'go', 'another', 'group', 'stealing', 'government', 'taxpayers', 'group', 'somalis', 'stole', 'four', 'million', 'government', 'benefits']\n"
          ]
        }
      ]
    }
  ]
}